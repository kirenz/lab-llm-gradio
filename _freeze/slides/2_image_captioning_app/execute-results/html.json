{
  "hash": "10cb46d8d36b05a59640984a845d2951",
  "result": {
    "markdown": "---\ntitle: Gradio Image Captioning App \ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: Gradio Tutorial 2\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Image Captioning App \n\nIn this tutorial, you'll create an image captioning app with a Gradio interface.\n\n*This tutorial is mainly based on an excellent course provided by Isa Fulford from OpenAI and Andrew Ng from DeepLearning.AI.*\n\n## Setup\n\n## Python\n\nLoad your HF API key and relevant Python libraries.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport gradio as gr\nimport os\nimport io\nimport requests\nimport json\nimport IPython.display\nfrom PIL import Image\nimport base64\nfrom transformers import pipeline\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())  # read local .env file\nhf_api_key = os.environ['HF_API_KEY']\n```\n:::\n\n\n## Image captioning model\n\nHere we'll be using the Salesforce/blip-image-captioning-base a 14M parameter captioning model.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nget_completion = pipeline(\n    \"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n\n\ndef summarize(input):\n    output = get_completion(input)\n    return output[0]['generated_text']\n\n```\n:::\n\n\n## API-code (optional) {.smaller}\n\n- Optional: Here is the code for the API-version:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Image-to-text endpoint (with API)\n# def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_ITT_BASE']):\n#     headers = {\n#       \"Authorization\": f\"Bearer {hf_api_key}\",\n#       \"Content-Type\": \"application/json\"\n#     }\n#     data = { \"inputs\": inputs }\n#     if parameters is not None:\n#         data.update({\"parameters\": parameters})\n#     response = requests.request(\"POST\",\n#                                 ENDPOINT_URL,\n#                                 headers=headers,\n#                                 data=json.dumps(data))\n#     return json.loads(response.content.decode(\"utf-8\"))\n```\n:::\n\n\n# Image Captioning\n\n## Image example\n\n- Free images are available on: https://free-images.com/\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimage_url = \"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg\"\n\ndisplay(IPython.display.Image(url=image_url))\n```\n:::\n\n\n## Model output\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nget_completion(image_url)\n```\n:::\n\n\n- [{'generated_text': 'a dog wearing a santa hat and a red scarf'}]\n\n## Captioning with gr.Interface()\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef captioner(input):\n    result = get_completion(input)\n    return result[0]['generated_text']\n\n\ngr.close_all()\ndemo = gr.Interface(fn=captioner,\n                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\n                    outputs=[gr.Textbox(label=\"Caption\")],\n                    title=\"Image Captioning with BLIP\",\n                    description=\"Caption any image using the BLIP model\",\n                    allow_flagging=\"never\"\n                    # examples=[\"your_image.jpeg\", \"your_image_2.jpeg\"]\n                    )\n\n\ndemo.launch(share=True)\n```\n:::\n\n\n## Gradio interface\n\n![](/images/image_app_1.png)\n\n## Gradio output\n\n![](/images/image_app_2.png)\n\n\n## API version (optional)\n\n- Optional: Use this code if you want to use the API\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# converts image to base 64 format (required for API)\n\n\ndef image_to_base64_str(pil_image):\n    byte_arr = io.BytesIO()\n    pil_image.save(byte_arr, format='PNG')\n    byte_arr = byte_arr.getvalue()\n    return str(base64.b64encode(byte_arr).decode('utf-8'))\n\n\ndef captioner(image):\n    base64_image = image_to_base64_str(image)\n    result = get_completion(base64_image)\n    return result[0]['generated_text']\n\n\ngr.close_all()\ndemo = gr.Interface(fn=captioner,\n                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\n                    outputs=[gr.Textbox(label=\"Caption\")],\n                    title=\"Image Captioning with BLIP\",\n                    description=\"Caption any image using the BLIP model\",\n                    allow_flagging=\"never\"\n                    # examples=[\"christmas_dog.jpeg\", \"bird_flight.jpeg\", \"cow.jpeg\"]\n                    )\n\n\ndemo.launch(share=True, server_port=int(os.environ['PORT1']))\n```\n:::\n\n\n## Close all connections\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ngr.close_all()\n```\n:::\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** üëç\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-llm-gradio/)**\n\n",
    "supporting": [
      "2_image_captioning_app_files"
    ],
    "filters": [],
    "includes": {}
  }
}