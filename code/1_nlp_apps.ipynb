{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NLP apps with a simple Gradio interface  \n",
                "\n",
                "In this tutorial, you'll create two NLP apps (text summarization and name entitiy recognition) with a Gradio interface and HuggingFace.\n",
                "\n",
                "*This tutorial is mainly based on an excellent course provided by Isa Fulford from OpenAI and Andrew Ng from DeepLearning.AI.*\n",
                "\n",
                "# Setup\n",
                "\n",
                "## Python"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import gradio as gr\n",
                "from transformers import pipeline\n",
                "import os\n",
                "import io\n",
                "from IPython.display import Image, display, HTML\n",
                "from PIL import Image\n",
                "import base64\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "_ = load_dotenv(find_dotenv()) \n",
                "\n",
                "hf_api_key = os.environ['HF_API_KEY']  # HuggingFace API"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HuggingFace\n",
                "\n",
                "## Basics\n",
                "\n",
                "- Hugging Face is a popular library for Natural Language Processing (NLP).\n",
                "- Provides easy-to-use interfaces known as `pipelines` for common NLP tasks.\n",
                "- Two main ways to use Hugging Face pipelines: locally or via the API.\n",
                "\n",
                "## Local Pipeline Usage {.smaller}\n",
                "\n",
                "```python\n",
                "from transformers import pipeline\n",
                "\n",
                "# Load a pipeline\n",
                "classifier = pipeline('sentiment-analysis')\n",
                "\n",
                "# Use the pipeline\n",
                "results = classifier('Hugging Face is creating a tool that democratizes AI.')\n",
                "```\n",
                "\n",
                "- Installation Required: Need to install Hugging Face Transformers library.\n",
                "\n",
                "- Data Privacy: Data remains on your local machine, ensuring privacy.\n",
                "\n",
                "- Cost: Free of charge\n",
                "\n",
                "- Performance: Performance dependent on your local machine's capabilities.\n",
                "\n",
                "- Customization: Easier to customize and tweak the pipeline.\n",
                "\n",
                "\n",
                "## Local Pipeline example\n",
                "\n",
                "```python\n",
                "from transformers import pipeline\n",
                "\n",
                "# Load a pipeline\n",
                "classifier = pipeline('sentiment-analysis')\n",
                "\n",
                "# Use the pipeline\n",
                "results = classifier('Hugging Face is creating a tool that democratizes AI.')\n",
                "```\n",
                "\n",
                "## API Pipeline Usage\n",
                "\n",
                "- No Installation required\n",
                "\n",
                "- Data Privacy Concerns: Data sent to Hugging Face servers, may not be suitable for sensitive data.\n",
                "\n",
                "- Cost: May incur charges depending on usage.\n",
                "\n",
                "- Performance: Performance dependent on network latency and Hugging Face's server capabilities.\n",
                "\n",
                "- Customization: Limited customization compared to local usage.\n",
                "\n",
                "## API Pipeline example\n",
                "\n",
                "```python\n",
                "import requests\n",
                "\n",
                "# Define the API endpoint and data\n",
                "url = 'https://api-inference.huggingface.co/pipeline/sentiment-analysis'\n",
                "headers = {'Authorization': 'Bearer YOUR_API_TOKEN'}\n",
                "data = {'inputs': 'Hugging Face is creating a tool that democratizes AI.'}\n",
                "\n",
                "# Make a request to the API\n",
                "response = requests.post(url, headers=headers, json=data)\n",
                "\n",
                "# Print the response\n",
                "print(response.json())\n",
                "```\n",
                "\n",
                "## Conclusion\n",
                "\n",
                "- Choosing between local and API pipeline usage depends on your project's requirements regarding data privacy, cost, performance, and customization.\n",
                "\n",
                "- Both methods offer a straightforward way to leverage the power of Hugging Face for NLP tasks.\n",
                "\n",
                "# Text Summarization App\n",
                "\n",
                "Building a text summarization app\n",
                "\n",
                "## Text summarization\n",
                "\n",
                "- To learn more about text summarization, take a look at [this tutorial](https://kirenz.github.io/huggingface/text-summarization/summarization-pipeline#/title-slide)\n",
                "\n",
                "\n",
                "## Helper function: local summarization pipeline"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "get_completion = pipeline(\n",
                "    \"summarization\", \n",
                "    model=\"sshleifer/distilbart-cnn-12-6\"\n",
                "    )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- We use the HuggingFace [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
                "\n",
                "\n",
                "## Helper function for API (optional) {smaller}\n",
                "\n",
                "- To run it via API, you could use an [Inference Endpoint](https://huggingface.co/inference-endpoints) for the `sshleifer/distilbart-cnn-12-6`, a 306M parameter distilled model from `facebook/bart-large-cnn`. \n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# # Helper function\n",
                "# import requests, json\n",
                "\n",
                "\n",
                "# #Summarization endpoint\n",
                "# def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']):\n",
                "#     headers = {\n",
                "#       \"Authorization\": f\"Bearer {hf_api_key}\",\n",
                "#       \"Content-Type\": \"application/json\"\n",
                "#     }\n",
                "#     data = { \"inputs\": inputs }\n",
                "#     if parameters is not None:\n",
                "#         data.update({\"parameters\": parameters})\n",
                "#     response = requests.request(\"POST\",\n",
                "#                                 ENDPOINT_URL, headers=headers,\n",
                "#                                 data=json.dumps(data)\n",
                "#                                )\n",
                "#     return json.loads(response.content.decode(\"utf-8\"))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text to summarize {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "text = ('''One of the best ways to share your machine learning model, API, or data science workflow with others is to create an interactive app that allows your users or colleagues to try out the demo in their browsers. Gradio allows you to build demos and share them, all in Python. And usually in just a few lines of code! Note that we shorten the imported name gradio to gr for better readability of code using Gradio. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it. You’ll also notice that in order to make apps, we create a gr.Interface. This Interface class can wrap any Python function with a user interface. The core Interface class is initialized with three required parameters: fn: the function to wrap a UI around; inputs: which component(s) to use for the input (e.g. \"text\", \"image\" or \"audio\"); outputs: which component(s) to use for the output (e.g. \"text\", \"image\" or \"label\") ''')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summarization example {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def summarize(input):\n",
                "    output = get_completion(input)\n",
                "    return output[0]['summary_text']\n",
                "\n",
                "\n",
                "summarize(text)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- ' Gradio allows you to build demos and share them in just a few lines of code . The core Interface class is initialized with three required parameters: fn: the function to wrap a UI around; inputs: which component(s) to use for the input (e.g. \"text\", \"image\" or \"audio) or \"label\")'\n",
                "\n",
                "## Gradio app code {.smaller}\n",
                "\n",
                "- Getting started with Gradio `gr.Interface` \n",
                "- `demo.launch(share=True)` creates a public link to share the app.\n",
                "- If you want to use the API-version, replace `demo.launch(share=False)` with `demo.launch(share=True, server_port=int(os.environ['PORT1']))`\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Helper function\n",
                "\n",
                "\n",
                "def summarize(input):\n",
                "    output = get_completion(input)\n",
                "    return output[0]['summary_text']\n",
                "\n",
                "\n",
                "# Close all current apps\n",
                "gr.close_all()\n",
                "\n",
                "# Start of the app\n",
                "demo = gr.Interface(\n",
                "    fn=summarize,\n",
                "    inputs=\"text\",\n",
                "    outputs=\"text\",\n",
                "    examples=[\"One of the best ways to share your machine learning model, API, or data science workflow with others is to create an interactive app that allows your users or colleagues to try out the demo in their browsers. Gradio allows you to build demos and share them, all in Python. And usually in just a few lines of code! Note that we shorten the imported name gradio to gr for better readability of code using Gradio. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it. You’ll also notice that in order to make apps, we create a gr.Interface. This Interface class can wrap any Python function with a user interface\"]\n",
                ")\n",
                "\n",
                "demo.launch(share=True)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio interface\n",
                "\n",
                "![](/images/gradio_app_1.png)\n",
                "\n",
                "\n",
                "## Gradio with text input\n",
                "\n",
                "![](/images/gradio_app_2.png)\n",
                "\n",
                "\n",
                "\n",
                "## Gradio with output\n",
                "\n",
                "![](/images/gradio_app_3.png)\n",
                "\n",
                "\n",
                "## Extended Gradio app code {.smaller}\n",
                "\n",
                "- We include more text in the user interface\n",
                "\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def summarize(input):\n",
                "    output = get_completion(input)\n",
                "    return output[0]['summary_text']\n",
                "\n",
                "\n",
                "gr.close_all()\n",
                "demo = gr.Interface(fn=summarize,\n",
                "                    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
                "                    outputs=[gr.Textbox(label=\"Result\", lines=3)],\n",
                "                    title=\"Text summarization with distilbart-cnn\",\n",
                "                    description=\"Summarize any text using the `sshleifer/distilbart-cnn-12-6` model under the hood!\"\n",
                "                    )\n",
                "\n",
                "demo.launch(share=True)\n",
                "\n",
                "# API-Version\n",
                "# demo.launch(share=True, server_port=int(os.environ['PORT2']))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extended Gradio output\n",
                "\n",
                "![](/images/gradio_app_4.png)\n",
                "\n",
                "\n",
                "# Named Entity Recognition App\n",
                "\n",
                "Building a Named Entity Recognition app\n",
                "\n",
                "## What is entity recognition?\n",
                "\n",
                "- Named entity recognition (NER): Find the entities (such as persons, locations, or organizations) in a sentence. \n",
                "\n",
                "- This can be formulated as attributing a label to each token by having one class per entity and one class for “no entity.”\n",
                "\n",
                "## Helper function: Named entity recognition pipeline\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper function for API-Version\n",
                "\n",
                "- If you want to use the [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task:\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
                "# get_completion(text, parameters=None, ENDPOINT_URL= API_URL)\n",
                "\n",
                "# API-Version\n",
                "# def ner(input):\n",
                "#     output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
                "#     return {\"text\": input, \"entities\": output}"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio NER App {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def ner(input):\n",
                "    output = get_completion(input)\n",
                "    return {\"text\": input, \"entities\": output}\n",
                "\n",
                "\n",
                "gr.close_all()\n",
                "\n",
                "demo = gr.Interface(fn=ner,\n",
                "                    inputs=[gr.Textbox(\n",
                "                        label=\"Text to find entities\", lines=2)],\n",
                "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
                "                    title=\"NER with dslim/bert-base-NER\",\n",
                "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
                "                    allow_flagging=\"never\",\n",
                "                    # Here we introduce a new tag, examples, easy to use examples for your application\n",
                "                    examples=[\"My name is Jan, I'm a professor at HdM Stuttgart and I live in Stuttgart\", \"My name is Lina and I study at HdM Stuttgart\"])\n",
                "\n",
                "demo.launch(share=True)\n",
                "\n",
                "# API-Version\n",
                "# demo.launch(share=True, server_port=int(os.environ['PORT3']))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## App interface\n",
                "\n",
                "![](/images/gradio_app_5.png)\n",
                "\n",
                "## App with output\n",
                "\n",
                "![](/images/gradio_app_6.png)\n",
                "\n",
                "\n",
                "\n",
                "## Gradio NER app with merged tokens"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def merge_tokens(tokens):\n",
                "    merged_tokens = []\n",
                "    for token in tokens:\n",
                "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
                "            # If current token continues the entity of the last one, merge them\n",
                "            last_token = merged_tokens[-1]\n",
                "            last_token['word'] += token['word'].replace('##', '')\n",
                "            last_token['end'] = token['end']\n",
                "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
                "        else:\n",
                "            # Otherwise, add the token to the list\n",
                "            merged_tokens.append(token)\n",
                "\n",
                "    return merged_tokens\n",
                "\n",
                "\n",
                "def ner(input):\n",
                "    output = get_completion(input)\n",
                "    merged_tokens = merge_tokens(output)\n",
                "    return {\"text\": input, \"entities\": merged_tokens}\n",
                "\n",
                "\n",
                "gr.close_all()\n",
                "demo = gr.Interface(fn=ner,\n",
                "                    inputs=[gr.Textbox(\n",
                "                        label=\"Text to find entities\", lines=2)],\n",
                "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
                "                    title=\"NER with dslim/bert-base-NER\",\n",
                "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
                "                    allow_flagging=\"never\",\n",
                "                    examples=[\"My name is Jan, I'm a professor at HdM Stuttgart and I live in Stuttgart\", \"My name is Lina, I live in Stuttgart and study at HdM Stuttgart\"])\n",
                "\n",
                "demo.launch(share=True)\n",
                "\n",
                "# API-Version\n",
                "# demo.launch(share=True, server_port=int(os.environ['PORT4']))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradio output\n",
                "\n",
                "![](/images/gradio_app_7.png)\n",
                "\n",
                "## Close all apps"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "gr.close_all()"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}