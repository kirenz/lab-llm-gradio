---
title: Simple NLP Apps
title-slide-attributes:
  data-background-image: ../images/logo.png
  data-background-size: contain
  data-background-opacity: "0.5"
lang: en
subtitle: Tutorial 6
author: Jan Kirenz
execute:
  eval: true
  echo: true
highlight-style: github
format:
  revealjs: 
    toc: true
    toc-depth: 1
    embed-resources: false
    theme: [dark, ../custom.scss]  
    incremental: true
    transition: slide
    background-transition: fade
    transition-speed: slow
    code-copy: true
    code-line-numbers: true
    smaller: false
    scrollable: true
    slide-number: c
    preview-links: auto
    chalkboard: 
      buttons: false
   # logo: ../images/logo.png
    footer: Jan Kirenz
---

# NLP apps with a simple interface  


# Setup


## Python

```{python}
import gradio as gr
from transformers import pipeline
import os
import io
from IPython.display import Image, display, HTML
from PIL import Image
import base64
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())  # read local .env file
hf_api_key = os.environ['HF_API_KEY']
```

## Helper function

```{python}
# running it locally

get_completion = pipeline(
    "summarization", model="sshleifer/distilbart-cnn-12-6")


def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']
```

## API {.smaller}

- The code would look very similar if you were running it from an API instead of locally. 
- The same is true for all the models in the rest of the course, make sure to check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page
- Alternatively to run it locally, you could use an [Inference Endpoint](https://huggingface.co/inference-endpoints) for the `sshleifer/distilbart-cnn-12-6`, a 306M parameter distilled model from `facebook/bart-large-cnn`. 

```{python}
# # Helper function
# import requests, json


# #Summarization endpoint
# def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']):
#     headers = {
#       "Authorization": f"Bearer {hf_api_key}",
#       "Content-Type": "application/json"
#     }
#     data = { "inputs": inputs }
#     if parameters is not None:
#         data.update({"parameters": parameters})
#     response = requests.request("POST",
#                                 ENDPOINT_URL, headers=headers,
#                                 data=json.dumps(data)
#                                )
#     return json.loads(response.content.decode("utf-8"))
```

# Building a text summarization app

```{python}
text = ('''The tower is 324 metres (1,063 ft) tall, about the same height
        as an 81-storey building, and the tallest structure in Paris. 
        Its base is square, measuring 125 metres (410 ft) on each side. 
        During its construction, the Eiffel Tower surpassed the Washington 
        Monument to become the tallest man-made structure in the world,
        a title it held for 41 years until the Chrysler Building
        in New York City was finished in 1930. It was the first structure 
        to reach a height of 300 metres. Due to the addition of a broadcasting 
        aerial at the top of the tower in 1957, it is now taller than the 
        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the 
        Eiffel Tower is the second tallest free-standing structure in France 
        after the Millau Viaduct.''')

get_completion(text)
```

### Getting started with Gradio `gr.Interface` 

#### How about running it locally?
The code would look very similar if you were running it locally.  Simply remove all the paramters in the launch method

```py
demo.launch()
```

`
``{python}

def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']


gr.close_all()
demo = gr.Interface(fn=summarize, inputs="text", outputs="text")
demo.launch(share=False)

# if you use the API-version
# demo.launch(share=True, server_port=int(os.environ['PORT1']))
```

You can add `demo.launch(share=True)` to create a public link to share with your team or friends.

```{python}


def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']


gr.close_all()
demo = gr.Interface(fn=summarize,
                    inputs=[gr.Textbox(label="Text to summarize", lines=6)],
                    outputs=[gr.Textbox(label="Result", lines=3)],
                    title="Text summarization with distilbart-cnn",
                    description="Summarize any text using the `sshleifer/distilbart-cnn-12-6` model under the hood!"
                    )

demo.launch(share=True)

# API-Version
# demo.launch(share=True, server_port=int(os.environ['PORT2']))
```

## Building a Named Entity Recognition app

If you want to use the [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task.

```{python}
# API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint
# get_completion(text, parameters=None, ENDPOINT_URL= API_URL)
```

We run it locally:

```{python}

text = "My name is Andrew, I'm building DeepLearningAI and I live in California"


get_completion = pipeline("ner", model="dslim/bert-base-NER")


def ner(input):
    output = get_completion(input)
    return {"text": input, "entities": output}


# API-Version
# def ner(input):
#     output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
#     return {"text": input, "entities": output}
```

```{python}
gr.close_all()

demo = gr.Interface(fn=ner,
                    inputs=[gr.Textbox(
                        label="Text to find entities", lines=2)],
                    outputs=[gr.HighlightedText(label="Text with entities")],
                    title="NER with dslim/bert-base-NER",
                    description="Find entities using the `dslim/bert-base-NER` model under the hood!",
                    allow_flagging="never",
                    # Here we introduce a new tag, examples, easy to use examples for your application
                    examples=["My name is Andrew and I live in California", "My name is Poli and work at HuggingFace"])

demo.launch(share=False)


# API-Version
# demo.launch(share=True, server_port=int(os.environ['PORT3']))
```

### Adding a helper function to merge tokens

```{python}
def merge_tokens(tokens):
    merged_tokens = []
    for token in tokens:
        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):
            # If current token continues the entity of the last one, merge them
            last_token = merged_tokens[-1]
            last_token['word'] += token['word'].replace('##', '')
            last_token['end'] = token['end']
            last_token['score'] = (last_token['score'] + token['score']) / 2
        else:
            # Otherwise, add the token to the list
            merged_tokens.append(token)

    return merged_tokens


def ner(input):
    output = get_completion(input)
    merged_tokens = merge_tokens(output)
    return {"text": input, "entities": merged_tokens}


# API-Version
# def ner(input):
#     output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
#     merged_tokens = merge_tokens(output)
#     return {"text": input, "entities": merged_tokens}

gr.close_all()
demo = gr.Interface(fn=ner,
                    inputs=[gr.Textbox(
                        label="Text to find entities", lines=2)],
                    outputs=[gr.HighlightedText(label="Text with entities")],
                    title="NER with dslim/bert-base-NER",
                    description="Find entities using the `dslim/bert-base-NER` model under the hood!",
                    allow_flagging="never",
                    examples=["My name is Andrew, I'm building DeeplearningAI and I live in California", "My name is Poli, I live in Vienna and work at HuggingFace"])

demo.launch(share=True)

# API-Version
# demo.launch(share=True, server_port=int(os.environ['PORT4']))
```

```{python}
gr.close_all()
```

